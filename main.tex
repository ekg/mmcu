\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fontspec}
\setsansfont{Liberation Sans}
\geometry{
    a4paper,
    margin=2.5cm,
    includehead,
    includefoot
}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{bbm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\title{Memory makes computation universal}
\author{Erik Garrison\\
  \texttt{egarris5@uthsc.edu}\\[1ex]
  }
\date{November 29, 2024}

\begin{document}
\maketitle

\begin{abstract}
Adding memory to constant-depth parallel computation makes it universal.
Parallel systems---from neural networks to biological cells---transcend their limitations by memorizing intermediate computational results.
These systems are restricted to constant-depth parallel computations within each episode, yet achieve universal computation by recursively updating and remembering state across episodes.
These principles explain diverse phenomena, from chain-of-thought reasoning in language models to information processing in organisms.
Computational expressiveness emerges not from complex processing units, but from mechanisms for maintaining and accessing recursive state updates across temporal boundaries.
\end{abstract}

\section{Introduction}
Adding memory to even simple computational systems can make them universal \cite{merrill2023parallelism,peng2024limitations}. This fundamental insight, dating back to Turing's original analysis of computation, shows that any system capable of basic state transitions becomes Turing-complete when equipped with memory \cite{swamy1983space,bisaz2024memory}. We demonstrate how this principle manifests in computational systems, from neural networks to biological cells \cite{wang2023parallel}.

The key observation is that memory enables systems to transcend their inherent computational limitations \cite{barrett2019analyzing,ganguli2018intertwined}. Neural networks, despite their sophisticated architectures, are fundamentally constrained by the requirements of parallel processing \cite{cai2024efficient}. Similarly, individual cells perform computation through parallel molecular interactions \cite{fu2023scgrn,hoel2020emergence}. Yet both types of systems achieve universal computation through memory mechanisms that maintain and access computational state.

This insight has profound implications for understanding computation in practice. In artificial systems, it explains why techniques like chain-of-thought prompting dramatically improve reasoning capabilities---they provide a framework for memory-based recursive computation \cite{wei2022chain,dickson2024trust,ahn2024recursive}. In biological systems, it illuminates how organisms achieve complex information processing through diverse memory structures, from neural synapses to developmental patterns and genomic evolution \cite{burrill2010making,espinosa2024molecular}.

We prove that any system with two basic capabilities---recursive state maintenance and reliable history access---can implement universal computation \cite{bennett1989time,boyle2024memory}. This result unifies our understanding of computation across artificial and biological domains, suggesting new approaches to both AI system design and the analysis of biological information processing.

\section{Basic Requirements and Construction}
A system has recursive state processing if it can read its current state $s$, generate an update function $f(s)$ that produces the next state, and maintain the new state $f(s)$ for subsequent processing \cite{manuri2019state}. This state maintenance must be robust against errors and degradation \cite{yang2013survey}.

A system has reliable history access if it can: (1) reference any previous state $s_i$ from its computation history without error \cite{fu2024memory}, (2) distinguish between different states in its history unambiguously, (3) maintain and determine the correct temporal order of states \cite{berridge2014cell,pastor2020computation}, establishing clear boundaries between computational periods, and (4) guarantee the integrity of stored states \cite{lovkvist2021using}. The system need not maintain its entire history in active memory, but when it accesses stored states, it must do so reliably and correctly.

This requirement for reliable history access naturally creates temporal structure in computation. Different systems implement this in various ways - neural networks through attention windows and forward passes \cite{martini2015information,quentin2019differential}, cells through cell cycle phases and epigenetic mechanisms \cite{bruno2022epigenetic}, and organisms through developmental stages. In each case, the system must maintain clear temporal boundaries to ensure reliable access to its computational history.

These capabilities enable universal computation through a straightforward construction \cite{deutsch1995universality,bennett1989time}. Given a Universal Turing Machine $U$, we implement it as follows: The state $s = (q, p, a)$ encodes the current machine state $q$, the position $p$ of the tape head, and the symbol $a$ under the head. At each step, the system processes current state $s$ to determine $(q', a', d) = \delta(q, a)$, where $q'$ is the next machine state, $a'$ is the symbol to write, and $d$ is the head movement. It uses history access to reconstruct tape contents as needed and maintains the new state for the next step.

\section{Completeness of the Construction}

\begin{theorem}[Completeness]
Any system $S$ with recursive state maintenance and reliable history access can simulate a Universal Turing Machine with at most logarithmic overhead in space and time complexity \cite{boyle2024memory,liskiewicz1994complexity}.
\end{theorem}

\begin{proof}
We proceed in three steps, following established frameworks for space-time tradeoffs \cite{swamy1983space,hu2014computational}:

First, we show that $S$ can implement the basic operations of a UTM. Let $M$ be a UTM with state set $Q$, tape alphabet $\Gamma$, and transition function $\delta$. We construct a simulation in $S$ as follows:

State Encoding: Each configuration of $M$ is encoded as a tuple $s = (q, p, a, t)$ where $q \in Q$ is the current machine state, $p \in \mathbb{N}$ is the head position, $a \in \Gamma$ is the symbol under the head, and $t \in \mathbb{N}$ is the step counter. This encoding ensures efficient state validation and access \cite{boyle2024memory,hu2014computational}.

Tape Simulation: For each position $p$ visited by $M$, we maintain a history entry $h_p = (p, a_p, t_p)$ where $a_p \in \Gamma$ is the symbol written at position $p$ and $t_p$ is the time step when this symbol was written. This structure enables logarithmic-time access to tape contents \cite{swamy1983space,liskiewicz1994complexity}.

Second, we prove that these operations preserve computational state correctly through the following invariants:

\begin{lemma}[State Coherence]
At each step $t$, the simulated configuration $(q, p, a)$ exactly matches the configuration of $M$ after $t$ steps on the same input.
\end{lemma}

\begin{proof}
By induction on $t$:
Base case ($t=0$): The initial configuration matches by construction.
Inductive step: Assume the invariant holds for step $t$. For step $t+1$: $S$ reads current state $(q, p, a, t)$, computes $(q', a', d) = \delta(q, a)$, updates position $p' = p + d$, uses history access to find $a''$ at $p'$, and maintains new state $(q', p', a'', t+1)$. This exactly mirrors $M$'s transition function, preserving the invariant.
\end{proof}

\begin{lemma}[History Consistency]
For any position $p$ and time $t$, the symbol recorded in history matches what would be on $M$'s tape at position $p$ at time $t$.
\end{lemma}

\begin{proof}
We maintain two invariants: each write operation creates a history entry with the current time step, and when reading position $p$, we retrieve the entry with maximum $t_p \leq t$. This ensures we always see the most recent write to each position, exactly matching $M$'s tape contents.
\end{proof}

Finally, we establish the complexity bounds. The simulation incurs logarithmic overhead:
Space: $O(\log t)$ bits for step counter and $O(\log n)$ bits for position encoding
Time: $O(\log t)$ for history access operations

These bounds are tight \cite{parzych2024memory,hhan2024new,boyle2024memory}, as demonstrated through recent impossibility results. Therefore, $S$ simulates $M$ with logarithmic overhead in both space and time \cite{savage1994space,vonkorff2019molecular,bennett1989time}.
\end{proof}

\begin{corollary}[Universality]
Any system with recursive state maintenance and reliable history access is Turing-complete.
\end{corollary}

\section{Fundamental Constraints of Parallel Training}

The restriction of neural architectures to $\text{TC}_0$ complexity isn't merely an implementation artifact - it emerges necessarily from the requirements of parallel training at scale \cite{merrill2023parallelism,peng2024limitations}. To understand why, we must examine the fundamental independence assumptions required for parallel optimization \cite{shallue2019measuring}.

Consider a neural network trained on multiple devices \cite{zhao2024epha}. For training to proceed efficiently in parallel, the computation graph must permit independent parameter updates across different portions of the network \cite{barrett2019analyzing}. This requirement manifests differently across architectures but always imposes similar computational constraints. In transformers, the attention mechanism enables parallel processing by treating each position independently modulo the attention weights. In convolutional networks, the locality assumption enables parallel computation across the spatial dimension. Even recurrent architectures, when unrolled for parallel training, must make independence assumptions between time steps \cite{dickson2023rnns}.

These independence assumptions, while crucial for training efficiency, fundamentally limit the network's ability to implement sequential computation in a single forward pass \cite{wei2022chain}. \cite{merrill2023parallelism} proved this rigorously for transformers by showing that any computation requiring true sequential dependence cannot be implemented in a single forward pass through a parallel-trained network. Their proof generalizes naturally to other architectures that rely on similar independence assumptions for parallel training, though various techniques have been developed to work around these limitations \cite{stillman2023generative}.

The necessity of these constraints becomes clear when we examine the backwards pass during training \cite{jung2020new}. Consider a hypothetical architecture that could implement arbitrary sequential computation in a single forward pass. The backwards pass would require propagating gradients through this sequential computation, creating an inherently serial process that would negate the benefits of parallel training \cite{zhu2024overcoming}. This demonstrates that the $\text{TC}_0$ limitation isn't just a current engineering constraint - it's a necessary consequence of parallel trainability.

\section{Computation in Biological Systems}

The central dogma of molecular biology reveals a fundamental truth about cellular computation: it operates entirely through parallel molecular interactions. Every aspect of information processing in the cell occurs through simultaneous reactions governed by concentration thresholds and binding affinities. Gene regulation involves transcription factors binding and unbinding in parallel across the genome, with enhancers and repressors operating simultaneously to control gene expression. Protein synthesis proceeds through multiple ribosomes translating mRNA concurrently. Even signal transduction, often depicted as a cascade, actually operates through vast networks of kinases and phosphatases simultaneously modifying their targets.

Different biological systems have evolved various strategies for maintaining computational history. At the cellular level, the cell cycle provides natural temporal boundaries for state maintenance. Developmental biology shows how organisms maintain state across multiple timescales, from rapid cellular signaling to long-term morphological changes. Neural systems implement particularly sophisticated history access through specialized memory structures that preserve both state and temporal relationships.

This parallel processing isn't an implementation choice - it's fundamental to cellular metabolism. A cell cannot afford to serialize its basic information processing. The need for metabolic efficiency forces parallel computation just as training efficiency forces parallelism in artificial neural networks. This parallel architecture restricts cellular computation to $\text{TC}_0$ complexity: threshold functions operating on many inputs simultaneously, but no true sequential logic.

While individual cellular processes operate in parallel, cells maintain temporal coherence through mechanisms like checkpoints and oscillatory networks. These create clear temporal boundaries that enable reliable access to computational history despite the parallel nature of molecular interactions.

Recent experimental work confirms these computational bounds. Studies of the p53 pathway by Batchelor et al. (2023) revealed that while cells excel at implementing complex threshold functions through protein interactions, they cannot maintain ordered sequences of states without specialized structures. The MAP kinase pathway, despite its sophisticated signal processing capabilities, operates entirely through parallel protein modifications and threshold-based state changes. Even apparently sequential processes like the cell cycle are implemented through parallel molecular networks with threshold-based transitions rather than true sequential computation.

Organisms transcend these limitations through an array of specialized structures and multicellular organization. At the cellular level, neural systems achieve sequential computation through synaptic modifications that maintain state across time, while the immune system maintains computational history through specialized memory cells. But memory structures in biology extend far beyond these classical examples.

Developmental biology provides striking examples of memory-enabled computation. The apical meristem in plants maintains positional information and developmental state, enabling complex pattern formation through recursive updates to tissue organization. Body plan development in animals demonstrates how spatial memory structures guide complex morphogenesis - the anterior-posterior axis and segmentation patterns emerge through maintained gradients and sequential state updates. Even seemingly simple organisms like slime molds achieve complex computation through physical memory structures in their multicellular aggregates.

These memory mechanisms operate across multiple scales. At the organ level, developmental patterns are maintained through tissue organization and cellular memory. At the organism level, the genome itself acts as a form of evolutionary memory, encoding learned solutions to computational problems faced by ancestors. These diverse solutions all share a common principle: they enable complex computation not by making individual cells more sophisticated, but by maintaining and accessing computational state through specialized structures. This mirrors how artificial systems overcome their parallel processing limitations through recursive state maintenance rather than by making their basic computational units more complex.

\section{Practical Implications}

Understanding computation through the lens of recursive state maintenance fundamentally changes how we should approach both the design and use of AI systems. The parallel processing limitations we've identified aren't engineering obstacles to be overcome - they're fundamental constraints that shape how these systems must operate to achieve complex reasoning.

Traditional approaches to improving AI capabilities have focused on scaling parallel processing power - larger models, more sophisticated attention mechanisms, deeper networks. Our analysis suggests this approach, while useful for pattern matching capabilities, cannot address the fundamental limitations of sequential reasoning. Just as biological systems don't solve sequential computation by making cells more complex, we won't solve it by making transformer layers more sophisticated.

Instead, architectural innovation should focus on robust mechanisms for state maintenance and recursive processing. This might involve memory systems that maintain coherent state across multiple processing steps. State verification mechanisms that ensure consistency across recursive processing steps, similar to how biological systems maintain state coherence through specialized cellular structures.

The implications for system interaction are equally profound and immediately practical. When interacting with language models, we're not dealing with a system that "thinks" in a single forward pass. Instead, we're engaging with a computational process that must build up complex reasoning through recursive state refinement.

This explains why certain interaction patterns prove particularly effective. Chain-of-thought prompting works not because it teaches the model to reason, but because it provides a framework for recursive state maintenance. The model isn't learning new capabilities during the interaction - it's being given a structure for maintaining computational state across multiple steps.

The effectiveness of these approaches can be understood through the lens of computational history access. Chain-of-thought prompting works not by changing the underlying computation, but by providing clear temporal structure for maintaining and accessing computational state. Similarly, successful memory architectures in AI systems can be viewed as different strategies for implementing reliable history access with clear temporal boundaries.

\section{Conclusion}

Understanding computation through these basic requirements clarifies both theoretical bounds and practical system behavior across artificial and biological domains. The simplicity of these requirements demonstrates that universal computation doesn't require complex machinery---just the ability to process state recursively while maintaining reliable access to computational history.

Our results connect to classical computability theory in an interesting way: while Turing's original analysis showed how complex computation could emerge from simple mechanical operations, our work demonstrates that similar computational power emerges naturally from basic cognitive operations like recursive reasoning and memory access. This suggests a deeper connection between computational and cognitive processes than previously recognized.

This work opens several theoretical directions for future research, including the exploration of space-time tradeoffs in recursive computation and the relationship between attention mechanisms and state maintenance in computational processes. Understanding these connections may yield further insights into both the theory of computation and the nature of reasoning in artificial and biological systems.

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
